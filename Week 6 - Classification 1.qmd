---
title: "Week 6 - Classification 1"
editor: visual
---

## Summary

Before the lecture, I felt nervous as machine learning and classification seemed challenging. However, unlike GEE, these techniques have long been used in remote sensing (Lary, 2015) to categorise pixels, objects, or patterns into thematic classes. Here is a very interesting example where a nonprofit organisation used satellite technology and machine learning to combat illegal fishing:

::: {style="aspect-ratio:16/9; width:100%; max-width:800px; margin:1rem auto; position:relative;"}
<iframe src="https://www.youtube.com/embed/Mz9xysVIrUU" title="Project eyes on the seas" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen style="position:absolute; inset:0; width:100%; height:100%; border:0;">

</iframe>
:::

### Machine learning:

Humans by nature use inductive learning: our brain provides us with information from experiences which we use to derive patterns. The more past experiences we have the better prediction we make.

Machine learning is the science of a computer modelling the human learning process.

There are various types of machine learning:

**Supervised learning:** Learning the function that relates an input and output using a training example given as input and output pairs.

**Unsupervised learning:** Learns patterns in a data set without reference to training data

**Below I break down some the methods we have discussed in the lecture in the best way I can.. Through a table (not all otherwise we'd be here forever):** <br>

| **Name** | **Type** | **Method** | **Strengths** | **Limitations** |
|----|----|----|----|----|
| **Classification and Regression Trees (CART)** | Supervised | Builds a decision tree by splitting data into groups based on key features; final leaves give predicted class (categorical) or value (continuous). | Easy to understand & interpret; handles both categorical & continuous data. | Prone to overfitting; unstable to small changes; lower accuracy than ensembles (e.g., Random Forests). |
| **Random Forests** | Supervised | Ensemble of CART trees built from bootstrap samples and random feature subsets; predictions combined by majority vote (classification) or averaging (regression). | High accuracy & robustness; handles large datasets; reduces overfitting. | Less interpretable (“black box”); computationally heavier; requires parameter tuning. |
| **k-means** | Unsupervised | Groups pixels into *k* clusters by assigning them to the nearest cluster center and adjusting centers until stable. | Simple & fast; works well with compact, well-separated clusters; no training data needed. | Must predefine *k*; sensitive to initial placement; struggles with irregular or noisy clusters. |

<br>

**Some key points I took from this week's lecture:**

With higher accuracy we often lose interpretability of our models.

![](machine_learning.png){fig-align="center"}

**Source:** [SHEYKHMOUSA et al. 2020 Support Vector Machine Versus Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review](https://www.researchgate.net/figure/Four-causes-of-mixed-pixels_fig3_242103275)

When running decision trees - we can see every decision that it makes.

Whereas with Random Forests - we don’t know predictions are made because while they are often accurate these models are also very complex.

## Applications:

### Supervised method:

![Framework of the proposed method for mangrove ecosystem mapping using the random forest (RF)](mangrove.jpg){fig-align="center"}

Ghorbanian et al. (2021), mapped the Hara mangrove ecosystem using Sentinel-1 synthetic aperture radar data and Sentinel-2 optical data within Google Earth Engine. Nine detailed mangrove classes were identified using precise reference samples from high-resolution imagery,divided into training and testing sets with cross-validation to ensure robustness. Seasonal features reflecting tidal variations were used with a Random Forest classifier, improving classification accuracy. The approach achieved high accuracy (93.23% overall accuracy, 0.92 Kappa coefficient) and consistency, demonstrating the effectiveness of combining multi-source data and seasonal downscaling for detailed, automated, and repeatable mangrove ecosystem mapping worldwide. Supervised classification was ideal due to difficult field access, necessitating reliable reference data from imagery, and the need to differentiate detailed mangrove classes critical for ecosystem monitoring.

### Unsupervised:

On the other hand Rapinela et al. (2019) used an unsupervised hierarchical TWINSPAN classifier to group vegetation relevé data into plant community types. TWINSPAN worked by repeatedly splitting the data into clusters based on species composition and identifying indicator species that defined each division. This process produced a hierarchical classification tree of vegetation units, with each relevé assigned to a cluster of similar species assemblages. The quality of these clusters was then assessed using silhouette widths, where values close to 1 indicated strong cohesion within clusters and clear separation from others. This method effectively identified consistent, locally relevant plant communities in wet grasslands, avoiding the limitations of broad, continental-scale habitat maps that often overlook fine-scale ecological patterns.

![Map of plant communities derived from the classification of Sentinel-2 time-series using a linear kernel support vector machine classifier.](Unsupervised.png){fig-align="center" width="274"}

Ultimately, Ghorbanian et al. (2021) showed supervised methods achieve high accuracy but rely on reference data and heavy computation. Rapinela et al. (2019) highlighted that unsupervised approaches avoid this but are less transferable. Future work could explore hybrid or semi-supervised methods (Which I’m sure we will cover next week), combining the precision of supervised models with the adaptability of unsupervised ones, supported by advances in transfer learning and cloud-based platforms.

## **Reflection:**

Machine learning has always felt inaccessible, overly technical, and far removed from practical application. However, this week changed my perspective. Rather than seeing algorithms as abstract, I began to recognise them as structured ways of organising uncertainty, enabling us to generate knowledge from complex datasets. This allowed me to move from nervousness toward curiosity.

What I found most engaging was the relevance of these methods across very different contexts. A week before the lecture, I had already tried applying K-means clustering on census data to group LSOAs in Tower Hamlets that share characteristics of poor recycling according to[ReLondon (The Mayor of London’s Recycling Team)](https://relondon.gov.uk/increase-recycling). I admit that I wasn't really 100% sure what I was doing so I relied heavily on google. This was the output:

![](Recycling.png){fig-align="center" width="681"}

but on reflection I now recognise that K-means may not be the most appropriate method for polygon data, as it is better suited to continuous feature space rather than spatial units with irregular boundaries.

I also started to think about other ways I could apply classification and remote sensing to work. For instance, supervised learning might help map green space distribution or track patterns of development, while unsupervised approaches could identify emerging trends without the burden of extensive training data. Such possibilities suggest real scope for evidence based decision making at a local level.

The lecture also raised a critical methodological point. If training and validation data are located too closely together. This spatial dependency can lead to inflated accuracy, effectively giving the model a “sneak peek” at validation data. Unless this is accounted for through techniques such as spatial cross validation or block sampling, results may appear stronger than they truly are. The risk is particularly important in policy contexts, where misleading accuracy could undermine trust or lead to poor decisions.

Overall, I think that this week was less about memorising algorithms and more about understanding their implications. Machine learning is not simply a technical skillset but a bridge between disciplines. It holds potential for environmental monitoring, urban planning, and social policy, provided it is applied with both rigour and transparency.
