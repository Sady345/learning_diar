[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary",
    "section": "",
    "text": "Welcome\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Week 1 - Getting started with remote sensing.html",
    "href": "Week 1 - Getting started with remote sensing.html",
    "title": "1  Week 1 - Getting started with remote sensing",
    "section": "",
    "text": "1.1 Summary:\nGoing into the first week of remote sensing I wasn’t sure what to expect. All I knew is that it had something to do with satelites and people way smarter than me.\nThis week I learned that remote sensing is a powerful method of collecting data by measuring the reflected and emitted radiation of an area (USGS, 2024). The sensors used for gathering this data can be mounted on various platforms including satellites, planes, drones and even… mobile phones.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1 - Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Week 1 - Getting started with remote sensing.html#summary",
    "href": "Week 1 - Getting started with remote sensing.html#summary",
    "title": "1  Week 1 - Getting started with remote sensing",
    "section": "",
    "text": "1.1.1 Remote Sensing Applications in Earth and Ocean Observations\n\n\n\n\n\n\nSatellite and Airplane Imaging\n\n\n\n\n\nCameras on satellites and airplanes take images of large areas on the Earth’s surface. This technology provides the ability to monitor vast landscapes and track changes over time, which is especially useful for environmental studies, urban planning, and disaster management.\n\n\n\n\n\n\n\n\n\n\nSonar Systems for Ocean Floor Imaging\n\n\n\n\n\nSonar systems on ships can be used to create images of the ocean floor without needing to travel to the bottom of the ocean. These systems emit sound waves that bounce off the ocean floor, providing detailed images of underwater features, such as seafloor topography and submerged objects, aiding navigation and marine research.\n\n\n\n\n\n\n\n\n\n\nSatellite Imaging for Marine Biological and Physical Variables\n\n\n\n\n\nCameras on satellites can be used to make images of biological and physical variables in the oceans. This includes observing sea surface temperature, chlorophyll-a concentrations, and ocean color. These images allow for the monitoring of marine ecosystems, tracking biological productivity, and understanding the impacts of climate change on ocean health.\n\n\n\n\n\n\n1.1.2 Spectral resolution\nDuring this week’s lecture, we covered spectral resolution, which I found interesting as it was a new concept to me.\nIt measures specific wavelengths of light across the electromagnetic spectrum (something I haven’t covered since GCSE physics..), which allows features like water, soil, and vegetation to be distinguished by their unique spectral pattern.\n\n\n\n\n\nDifferent sensors are designed with different spectral resolutions.\n\n\n\n\n\nMultispectral sensors capture a few broad spectral bands (3–10), mainly visible light and some non-visible bands including infrared. They are simple so therefore cost-effective and suitable for large-scale monitoring such as distinguishing forest from water, but their broad bands can obscure finer differences, such as between tree species or in water quality.\nHyperspectral imaging captures data in hundreds of narrow adjacent spectral bands. This allows for finer distinctions, such as between tree species or specific water pollutants. However, this detail comes at the cost of greater complexity and large datasets that are more demanding to process. Due to this, hyperspectral imaging is less suitable for continuous global monitoring.\n\n\n1.1.3 What I make of this:\nI currently work for the London Borough of Tower Hamlets, where the mayor has declared a waste emergency and the borough is currently the worst in the country for recycling. Hyperspectral imagery would be very useful in this case to potentially detect and classify the types of contamination present in recycling loads. However, this would require th purchasing of a hyperspectral cameria. Likewuse, I don’t know much about classification yet so I’ll see if i still think this in week 6…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1 - Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Week 1 - Getting started with remote sensing.html#applications",
    "href": "Week 1 - Getting started with remote sensing.html#applications",
    "title": "1  Week 1 - Getting started with remote sensing",
    "section": "1.2 Applications:",
    "text": "1.2 Applications:\nIn the practical we used data from two multispectral sensors (Landsat 8 & Sentinel 2).\n\n\n\n\n\n\nLandsat 8\n\n\n\n\n\nLandsat satellites detect earth data while in orbit at a 30m resolution and have been providing the longest continuous Earth Observation record dating back to 1972.\n\n\n\n\n\n\n\n\n\nSentinel 2:\n\n\n\n\n\nWhereas, Sentinel 2 (Beginning 2015) are a pair of satellites that monitor the earth’s land surface at a higher resolution (10-60m).\n\n\n\nFrom the practical, the differences between Sentinel-2 and Landsat were clear. The contrast in imagery from the Landsat and Sentinel-2 can differ, and this is due to the differences in their sensor characteristics and processing methods. Landsat provides a 30 meter spatial resolution and Sentinel provides as high as 10 meters for certain bands. I was really confused from the practical why the images were so different. However it’s also because you might not be able to get an image from the exact time and day for both satellites.\nNote: The Sentinel-2 data was resampled to 30m to match landsat\n\n\n\nBefore colour manipulation in snap\n\n\nThis is how it looks after I used the colour manipulation which is adjusting how spectral bands are displayed as colours. A bit like using a snapchat filter the image is the same, but we can change the colours to highlight different details.\n\n\n\nAfter colour manipulation in SNAP\n\n\n\n1.2.1 Combining Landsat and Sentinel-2 data to assess burn severity\nQuintano et al. demonstrated that combining landsat and Sentinel-2 data can significantly enhance research. The study used Landsat 8 images taken before the fire to measure vegetation health using the Normalised Burn Ratio, then compared them with images taken after the fire from either Landsat or Sentinel-2. This showed how much the vegetation changed, allowing the resarchers to map the severity of the burn in a consistent way.\nLandsat offered a long-term archive and established indices for assessing burn severity. While Sentinel-2’s higher spatial resolution and increased revisit frequency allowed more detailed assessments and quicker data availability, which are particularly valuable during fire emergencies. Likewise, when pre-fire Landsat data was unavailable, Sentinel-2 alone still offered a useful approximation of fire impact. The researchers suggest that future work should focus on fine-tuning these methods to improve accuracy, especially for less severely affected areas, which would help rescue teams and forest managers respond more effectively to fire emergencies.\nHowever this integrated approach is a great step to enhancing fire management by offering a more immediate and accurate understanding of the extent and severity of the burn.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1 - Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Week 1 - Getting started with remote sensing.html#reflection",
    "href": "Week 1 - Getting started with remote sensing.html#reflection",
    "title": "1  Week 1 - Getting started with remote sensing",
    "section": "1.3 Reflection:",
    "text": "1.3 Reflection:\nMy views on remote sensing have definitely changed . During my undergraduate degree I was always scared to choose any remote sensing modules as I was always warned about how difficult they were. This week, I began to see how it connects to everyday problems and decisions. The ability to extract patterns from imagery whether through hyperspectral or multispectral imagery made me think about how often we overlook information that is literally all around us, waiting to be seen.\nI found the contrast between different satellite systems interesting. Initially, I was frustrated by the inconsistencies in the imagery and assumed that I was doing something wrong. However, this helped me realise the importance of evaluating data critically rather than accepting it at face value. This feels relevant beyond remote sensing, reminding me that any dataset is shaped by how it was collected, processed, and presented.\nI’m looking forward to the upcoming weeks, to really get into detail about how remotely sensed data is currently used for research and policy. Even though my background is not in physics or earth sciences, I can already imagine links with waste management, and environmental policy. The possibility of using spectral patterns to identify waste materials makes me consider how remote sensing could provide valuable evidence for local councils. Though that also raises the question of data availability, there’s only so much open data out there and local governments do have a relatively limited budget. I’ll definitely look into whether the council has any access to remotely sensed data or if they have ever used remotely sensed data for analysis .",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1 - Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Week 2 - Xaringan and Quarto.html",
    "href": "Week 2 - Xaringan and Quarto.html",
    "title": "2  Week 2 - Xaringan and Quarto",
    "section": "",
    "text": "2.1 Presentation\nThis week we were taught how to use Xaringan to create presentations and Quarto to create our learning diary portfolio. This weeks entry contains a short presentation I have created on the hyperspectral sensor hyperion.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2 - Xaringan and Quarto</span>"
    ]
  },
  {
    "objectID": "Week 3 - corrections.html",
    "href": "Week 3 - corrections.html",
    "title": "3  Week 3 - Corrections",
    "section": "",
    "text": "3.1 This week I discovered that there are many ways to correct data….\nHonestly this is the most I have been confused in this module I am lost for words… so I will attempt to summarise it as best as I can.\nLuckily for me most of the data that I’d use would come analysis ready however in a situation where I am dealing with raw data I would need to correct it. I understand corrections to be adjusting raw sensor data to remove potential flaws in atmospheric conditions, spatial inaccuracies and topographical variations.\nMore often than not remotely sensed data contains flaws.\nA famous example is when the landsat scan line corrector failed, without the landsat corrector we got zig zags and a misaligned image.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3 - Corrections</span>"
    ]
  },
  {
    "objectID": "Week 3 - corrections.html#this-week-i-learned-that-there-are-many-ways-to-correct-data.",
    "href": "Week 3 - corrections.html#this-week-i-learned-that-there-are-many-ways-to-correct-data.",
    "title": "4  Week 3 - Corrections",
    "section": "",
    "text": "4.1.1 Examples:\nGeometric correction: refers to the process of correcting for distortions in satellite \nImagery caused by multiple different factors including height, speed and direction of the imaging platform. It includes selecting ground control points (points that do not change) and matching them to a gold standard image. OS is probably the best to use as a corrected image because it has highly accurate and regularly updated geographic reference points.\n\nAtmospheric correction: Atmospheric correction is always the first thing that you should do. It is the process of removing the effects of the Earth’s atmosphere on satellite or airborne imagery. Examples include dark object subtraction (DOS), pseudo-invariant features (PIFs) and empirical line correction.\n\nOrthorectification correction: is a subset of georectification and it creates a final product whereby each pixel in the image is depicted as if it were collected from directly overhead or as close to this as possible.\nRadiometric calibration: ensures that the satellite records light correctly by converting the recorded original digital number value into the reflectivity of the outer surface of the atmosphere.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 - Corrections</span>"
    ]
  },
  {
    "objectID": "Week 3 - corrections.html#application",
    "href": "Week 3 - corrections.html#application",
    "title": "3  Week 3 - Corrections",
    "section": "3.2 Application:",
    "text": "3.2 Application:\nThomas et al. (2019) applied Sentinel-2 imagery with machine learning to map wetlands and estimate biomass in Louisiana.The study employed the Dark Object Subtraction correction using the Semi-Automatic Classification plugin within QGIS.\nThe process involved identifying dark objects within the satellite imagery such as deep water, shadows or dense vegetation—that are known to have low reflectance across all spectral bands. The DOS algorithm then estimated the atmospheric haze based on these dark objects and subtracted this from the entire scene.\nWith less interference from the atmosphere, the surface reflectance became clearer. This allowed for better distinctions between different land cover types such as marshes, forested wetlands, and open water. This resulted in more precise segmentation and classification of vegetation and water bodies, which was particularly critical given the heterogeneous and dynamic nature of the wetlands in Louisiana. Ultimately, the correction contributed to more accurate estimates of wetland extent and composition, which are essential for monitoring changes over time and informing conservation efforts.\nOn the other hand Zhang et al. (2013), investigated the application of hyperspectral remote sensing combined with object-based image analysis (OBIA - which will be covered in later weeks) and machine learning algorithms to map benthic habitats in the Florida Keys. Specifically, they evaluated the effectiveness of AVIRIS hyperspectral imagery for automated habitat classification.\nThe study employed the FLAASH (Fast Line-of-Sight Atmospheric Analysis of Spectral Hypercubes) algorithm to perform atmospheric correction on the hyperspectral imagery. The correction was implemented within the ENVI software, a widely used tool for hyperspectral data analysis. This process involves estimating atmospheric conditions, applying models of how light interacts with the atmosphere, and then adjusting the data to remove atmospheric distortions.\nIn their analysis, the researchers compared classification results from data processed with and without atmospheric correction. They found that applying FLAASH slightly increased the total classification accuracy however, these improvements were not statistically significant. Therefore, although the correction helped remove atmospheric biases, its practical impact on classification accuracy in this context was minimal.\nThe contrast between these two studies suggests that the value of atmospheric correction cannot be assumed, but must be judged against sensor characteristics and ecosystem context.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3 - Corrections</span>"
    ]
  },
  {
    "objectID": "Week 3 - corrections.html#reflection",
    "href": "Week 3 - corrections.html#reflection",
    "title": "3  Week 3 - Corrections",
    "section": "3.3 Reflection:",
    "text": "3.3 Reflection:\nThe practical was quite long because it explored the many different ways to correct and enhance data. Also, I couldn’t complete the section that filtered and fused the data because it continuously crashed my r. I would only attempt this again if it was completely necessary and I had a computer with much better processing power. It will take a lot longer than a week to get around these concepts. However, if I am using raw data, potentially from two different satellites I now know how to perform radiometric calibration and ensure that there is consistency between the datasets (Lets hope I won’t have to anytime soon). This week I learned that there isn’t a one size fits all or a right or wrong answer. The type of correction chosen will depend on the use case and the data. Also sometimes a correction just isnt necessary as seen in the case of Zhang et al..",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3 - Corrections</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Week 1 - Getting started with remote sensing.html#imaging-methods",
    "href": "Week 1 - Getting started with remote sensing.html#imaging-methods",
    "title": "2  Week 1 - Getting started with remote sensing",
    "section": "2.2 Imaging Methods",
    "text": "2.2 Imaging Methods\n\n2.2.1 Satellite and Airplane Imaging\nCameras on satellites and airplanes take images of large areas on the Earth’s surface, allowing us to see much more than we can see when standing on the ground. This technology provides the ability to monitor vast landscapes and track changes over time, which is especially useful for environmental studies, urban planning, and disaster management.\n\n\n\nExample of Satellites\n\n\nSource: example.com\n\n\n2.2.2 Sonar Systems for Ocean Floor Imaging\nSonar systems on ships can be used to create images of the ocean floor without needing to travel to the bottom of the ocean. These systems emit sound waves that bounce off the ocean floor, providing detailed images of underwater features, such as seafloor topography and submerged objects, aiding navigation and marine research.\n\n\n\nExample of sonar systems\n\n\nSource: example.com\n\n\n2.2.3 Satellite Imaging for Marine Biological and Physical Variables\nCameras on satellites can be used to make images of biological and physical variables in the oceans. This includes observing sea surface temperature, chlorophyll-a concentrations, and ocean color. These images allow for the monitoring of marine ecosystems, tracking biological productivity, and understanding the impacts of climate change on ocean health.\n ### Why\nThis interests me because of the vast amount of available data. From the lecture, I learned that satellites revisit the same locations on Earth daily to every 16 days, continuously collecting data.\nIn the practical we used data from two multispectral sensors (Landsat 8 & Sentinel 2) which capture imagery across multiple wavelengths of the electromagnetic spectrum.\nMultispectral sensors typically capture data in a limited number of broad spectral bands (3-10). These bands are usually spaced further apart and correspond to visible light (e.g. red, green, blue) and a few non-visible bands including infrared.\nOn the other hand, hyperspectral imaging (which will be discussed further in week 2) captures data in a much larger number of narrow contiguous spectral bands (often hundreds).\n\n\n\nMultispectral vs Hyperspectral Imagery\n\n\nLandsat 8:\nLandsat satellites detect earth data while in orbit at a 30m resolution and have been providing the longest continuous Earth Observation record dating back to 1972.\nSentinel 2:\nWhereas, Sentinel 2 (Beginning 2015) are a pair of satellites that monitor the earth’s land surface at a higher resolution (10-60m).\n\n\n2.2.4 What I make of this:\nI currently work for the London Borough of Tower Hamlets and this could be very beneficial to identify areas where there is illegal dumping of waste. For instance, multispectral imagery might be able to detect different materials in waste, such as plastics, metals, or organic matter as these materials reflect and absorb light differently which may indicate illegal disposal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 - Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Week 1 - Getting started with remote sensing.html#remote-sensing-applications-in-earth-and-ocean-observations",
    "href": "Week 1 - Getting started with remote sensing.html#remote-sensing-applications-in-earth-and-ocean-observations",
    "title": "2  Week 1 - Getting started with remote sensing",
    "section": "2.2 Remote Sensing Applications in Earth and Ocean Observations",
    "text": "2.2 Remote Sensing Applications in Earth and Ocean Observations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 - Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Week 1 - Getting started with remote sensing.html#remote-sensing-applications-in-earth-and-ocean-observations-1",
    "href": "Week 1 - Getting started with remote sensing.html#remote-sensing-applications-in-earth-and-ocean-observations-1",
    "title": "2  Week 1 - Getting started with remote sensing",
    "section": "2.3 Remote Sensing Applications in Earth and Ocean Observations",
    "text": "2.3 Remote Sensing Applications in Earth and Ocean Observations\n\n\n\n\n\n\nSatellite and Airplane Imaging\n\n\n\n\n\nCameras on satellites and airplanes take images of large areas on the Earth’s surface, allowing us to see much more than we can see when standing on the ground. This technology provides the ability to monitor vast landscapes and track changes over time, which is especially useful for environmental studies, urban planning, and disaster management.\n\n\n\n\n\n\n\n\n\n\nSonar Systems for Ocean Floor Imaging\n\n\n\n\n\nSonar systems on ships can be used to create images of the ocean floor without needing to travel to the bottom of the ocean. These systems emit sound waves that bounce off the ocean floor, providing detailed images of underwater features, such as seafloor topography and submerged objects, aiding navigation and marine research.\n\n\n\n\n\n\n\n\n\n\nSatellite Imaging for Marine Biological and Physical Variables\n\n\n\n\n\nCameras on satellites can be used to make images of biological and physical variables in the oceans. This includes observing sea surface temperature, chlorophyll-a concentrations, and ocean color. These images allow for the monitoring of marine ecosystems, tracking biological productivity, and understanding the impacts of climate change on ocean health.\n\n\n\n\n\n2.3.1 Why\nThis interests me because of the vast amount of available data. From the lecture, I learned that satellites revisit the same locations on Earth daily to every 16 days, continuously collecting data.\nIn the practical we used data from two multispectral sensors (Landsat 8 & Sentinel 2) which capture imagery across multiple wavelengths of the electromagnetic spectrum.\nMultispectral sensors typically capture data in a limited number of broad spectral bands (3-10). These bands are usually spaced further apart and correspond to visible light (e.g. red, green, blue) and a few non-visible bands including infrared.\nOn the other hand, hyperspectral imaging (which will be discussed further in week 2) captures data in a much larger number of narrow contiguous spectral bands (often hundreds).\n\n\n\nMultispectral vs Hyperspectral Imagery\n\n\nLandsat 8:\nLandsat satellites detect earth data while in orbit at a 30m resolution and have been providing the longest continuous Earth Observation record dating back to 1972.\nSentinel 2:\nWhereas, Sentinel 2 (Beginning 2015) are a pair of satellites that monitor the earth’s land surface at a higher resolution (10-60m).\n\n\n2.3.2 What I make of this:\nI currently work for the London Borough of Tower Hamlets and this could be very beneficial to identify areas where there is illegal dumping of waste. For instance, multispectral imagery might be able to detect different materials in waste, such as plastics, metals, or organic matter as these materials reflect and absorb light differently which may indicate illegal disposal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 - Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Week 2 - Xaringan and Quarto.html#reflection",
    "href": "Week 2 - Xaringan and Quarto.html#reflection",
    "title": "2  Week 2 - Xaringan and Quarto",
    "section": "2.2 Reflection",
    "text": "2.2 Reflection\nAfter some time playing around with these tools I was able to do some really cool things like create panels in presentation slides (which ultimately reduces the ammounts of slides that need to be used). I can also see using quarto as being a large part of my career as a few of the data visualisations at the council are created using quarto markdown documents for instance the reporting of the annual resident survey results. Also, for some time now, the quarterly performance reports have been created manually in PowerPoint (which usually takes the whole quarter). Injecting the data into a qmd would definitely speed up the process and is something I am looking into now.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2 - Xaringan and Quarto</span>"
    ]
  },
  {
    "objectID": "Week 2 - Xaringan and Quarto.html#presentation",
    "href": "Week 2 - Xaringan and Quarto.html#presentation",
    "title": "2  Week 2 - Xaringan and Quarto",
    "section": "",
    "text": "⛶ Expand",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2 - Xaringan and Quarto</span>"
    ]
  },
  {
    "objectID": "Week 3 - corrections.html#this-week-i-discovered-that-there-are-many-ways-to-correct-data.",
    "href": "Week 3 - corrections.html#this-week-i-discovered-that-there-are-many-ways-to-correct-data.",
    "title": "3  Week 3 - Corrections",
    "section": "",
    "text": "3.1.1 Examples of corrections:\nIn this weeks lecture we covered numerous methods of correcting remotely sensed data and according to Google AI there are hundreds. so I will be going through the three main methods.\n\n\n\n\n\n\n\n\n\n\nMethod\nExplanation\nStrengths\nLimitations\nUse Case\n\n\n\n\nGeometric Correction\nCorrects distortions from imaging platform height, speed, and direction using ground control points and a reference image (e.g., OS maps).\nEnsures spatial accuracy; aligns with maps.\nNeeds reliable reference points; can take a lot of time.\nUrban planning; map alignment.\n\n\nAtmospheric Correction\nRemoves atmospheric effects (scattering, absorption). Techniques include dark object subtraction , pseudo-invariant features and empirical line correction.\nEnables cross-time and cross-sensor analysis.\nRequires assumptions about atmosphere; mistakes affect results.\nVegetation and land cover change.\n\n\nOrthorectification\nAdjusts for terrain/sensor geometry so pixels are as if viewed from directly overhead.\nProduces true map projection; accurate areas.\nNeeds high-quality Digital Elevation Models; computationally demanding.\nTopographic mapping; flood risk.\n\n\nRadiometric Calibration\nConverts raw digital numbers to surface reflectance values.\nConsistent brightness for spectral analysis.\nRequires calibration info for each sensor; still affected by lighting/atmosphere.\nClimate or water quality studies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3 - Corrections</span>"
    ]
  },
  {
    "objectID": "Week 4 - Palisades Fire.html",
    "href": "Week 4 - Palisades Fire.html",
    "title": "5  Week 4 - Palisades Fire",
    "section": "",
    "text": "5.1 Summary\nOn the 7th of January 2025 at approximately 10:30am a devastating fire swept through the Palisades neighborhood of Los Angeles California. After one week the fire had consumed nearly 24,000 acres of wildland and developed areas. More than 16,000 buildings were destroyed, 100,000 people evacuated and an estimated 440 have died as a result of these fires.\nHowever California has a long history of wildfires, dating back to the San Francisco Fire of 1851. Which begs the question, in this day and age with all the technology we have: is there a way of mitigating these destructive impacts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Palisades Fire</span>"
    ]
  },
  {
    "objectID": "Week 4 - Palisades Fire.html#summary",
    "href": "Week 4 - Palisades Fire.html#summary",
    "title": "5  Week 4 - Palisades Fire",
    "section": "",
    "text": "5.1.1 Policy:\nSenate Bill 1101 created by Senator Dave Limón allows the Department of Forestry and Fire Protection to bypass standard bidding rules when securing support for large prescribed fires. It requires the department to identify and map operational boundaries for wildfire response and prescribed burning by 2026. From 2025, the department must publish annual maps showing wildfire impact severity. It also requires an annual review of recent fires to identify priority areas for prescribed burning to reduce fuel hazards, protect communities, and support ecological restoration.\nLos Angeles County is developing the Community Wildfire Protection (CWP) Ordinance, a regulatory tool to reduce risks from wildfires in Very High Fire Hazard Severity Zones (VHFHSZ). Initiated by the Board of Supervisors in 2020, the ordinance builds on lessons from the Woolsey Fire After Action Review and expert guidance from the Community Planning Assistance for Wildfire (CPAW) programme.\nDevelopment of the ordinance is data-driven. It uses CAL FIRE hazard maps produced through the Fire and Resource Assessment Program (FRAP), which model wildfire risk based on fuels, terrain, and weather. These are paired with findings from the Woolsey Fire, CPAW’s land-use risk modelling, and community input through fire safe councils and local wildfire programmes. Together, these sources shape enforceable standards to make future communities more resilient.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Palisades Fire</span>"
    ]
  },
  {
    "objectID": "Week 4 - Palisades Fire.html#application",
    "href": "Week 4 - Palisades Fire.html#application",
    "title": "5  Week 4 - Palisades Fire",
    "section": "5.2 Application:",
    "text": "5.2 Application:\nFRAP already utilises remotely sensed data to monitor fires, designate hazard risk areas and evaluate the impacts of fires. In 2025 they developed the California Vegetation Burn Severity dataset (their latest dataset) to meet the requirements of the Senate Bill 1101. The dataset estimates fire severity in California between 2015 and 2023 using remotely sensed data. Fire perimeters were taken from FRAP’s Historic Wildland Fire Perimeters database and uploaded to Google Earth Engine (GEE). Within each perimeter, pre- and post-fire Landsat imagery (June–September period) were processed. These generated the Relative differenced Normalised Burn Ratio (RdNBR), which measures vegetation greenness change, and a bias-corrected Composite Burn Index (CBI), predicted from RdNBR using regression equations based on field plots.The resulting continuous rasters were clipped to fire boundaries (and to forest types for CBI), reclassified into severity classes, and converted to vector layers.\nAlthough the U.S. has extensive remote sensing systems for wildfire monitoring and response, their application in predicting, preventing wildfires, and supporting evacuations remains limited.\n\n5.2.0.1 I propose a methodology to support wildfire prediction and mitigation efforts using remotely sensed data:\nThis method uses Sentinel-2 vegetation indices to map areas with dry vegetation that could fuel wildfires, producing near real-time fuel stress maps and drought time series. VIIRS and Sentinel-1 track wildfire spread and direction, while building footprints, traffic, and population data identify high-risk housing areas and potential evacuation bottlenecks. By combining remote sensing with GIS datasets, hazard risks can be visualised to support mitigation (e.g., controlled burning) and adaptation (e.g., efficient evacuation planning)\n\n\n\n\n\nThis method builds on Senate Bill 1101 by making wildfire risk mapping more dynamic and responsive. Instead of relying on static hazard zones, it uses regularly updated satellite data to show changing conditions. This means communities can plan mitigation and evacuation using information that reflects current risk.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Palisades Fire</span>"
    ]
  },
  {
    "objectID": "Week 4 - Palisades Fire.html#reflection",
    "href": "Week 4 - Palisades Fire.html#reflection",
    "title": "5  Week 4 - Palisades Fire",
    "section": "5.3 Reflection:",
    "text": "5.3 Reflection:\nLA is a highly developed city and one of the United States’ largest economic engines. There are many good things you can say about LA despite the heat and traffic congestion. Therefore, they must have already developed a system that can predict wildfires, right? However, although extensive academic research has examined wildfire prediction through remotely sensed data (Sayad et al., 2019; Ntinopoulos et al., 2023), the city has yet to implement such methods, including the approach I proposed. While California employs remotely sensed data to evaluate wildfire impact and map hazard zones, the resulting product remains primarily static.\nUpon reflection, one potential improvement to my proposed approach would be the integration of machine learning and classification techniques to enhance wildfire prediction, as referenced in the methods above. Nonetheless, I believe this represents a strong first step toward producing a tangible outcome within a relatively short time frame, as it avoids the need for intensive analysis. Moreover, since FRAP has already utilized GEE for various visualisations, adopting this approach should be relatively feasible.\nFinally, while Senate Bill 1101 represents a step forward by requiring CAL FIRE to develop products, the bill leaves a major gap in how these mandates will be carried out. It sets ambitious deadlines but does not guarantee the funding, staffing, or technical resources necessary to implement them effectively. This risks creating a cycle where outputs such as maps and reports are produced to satisfy compliance, but without the capacity to translate them into tangible wildfire resilience. In this sense, the policy is strong on vision but not on execution, limiting its potential impact.\nWhat resonates with me from going through the policy, as an apprentice data scientist, is how often requests from senior leaders arrive with only broad direction. For instance, I might be asked to explore council priorities or measure returns on investment, but without clear guidance on what specifically should be examined or how results should be presented. This can leave analysts spending considerable time trying to interpret intentions rather than focusing on the analysis itself. Although those who do not work with remote sensing data may not grasp the technical details, more communication between decision makers and researchers can help bridge that gap. Leaders do not need to understand every step, but clearer goals and expectations make the analysis more relevant and useful.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Palisades Fire</span>"
    ]
  },
  {
    "objectID": "Week 4 - LA Fires.html",
    "href": "Week 4 - LA Fires.html",
    "title": "4  Week 4 - LA Fires",
    "section": "",
    "text": "4.1 Summary\nOn the 7th of January 2025 at approximately 10:30am a devastating fire swept through the Palisades neighborhood of Los Angeles California. After one week the fire had consumed nearly 24,000 acres of wildland and developed areas. More than 16,000 buildings were destroyed, 100,000 people evacuated and an estimated 440 have died as a result of these fires.\nHowever California has a long history of wildfires, dating back to the San Francisco Fire of 1851. Which begs the question, in this day and age with all the technology we have: is there a way of mitigating these destructive impacts.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4 - LA Fires</span>"
    ]
  },
  {
    "objectID": "Week 4 - LA Fires.html#summary",
    "href": "Week 4 - LA Fires.html#summary",
    "title": "4  Week 4 - LA Fires",
    "section": "",
    "text": "4.1.1 Policy:\nSenate Bill 1101 created by Senator Dave Limón allows the Department of Forestry and Fire Protection to bypass standard bidding rules when securing support for large prescribed fires. It requires the department to identify and map operational boundaries for wildfire response and prescribed burning by 2026. From 2025, the department must publish annual maps showing wildfire impact severity. It also requires an annual review of recent fires to identify priority areas for prescribed burning to reduce fuel hazards, protect communities, and support ecological restoration.\nLos Angeles County is developing the Community Wildfire Protection (CWP) Ordinance, a regulatory tool to reduce risks from wildfires in Very High Fire Hazard Severity Zones (VHFHSZ). Initiated by the Board of Supervisors in 2020, the ordinance builds on lessons from the Woolsey Fire After Action Review and expert guidance from the Community Planning Assistance for Wildfire (CPAW) programme.\nDevelopment of the ordinance is data-driven. It uses CAL FIRE hazard maps produced through the Fire and Resource Assessment Program (FRAP), which model wildfire risk based on fuels, terrain, and weather. These are paired with findings from the Woolsey Fire, CPAW’s land-use risk modelling, and community input through fire safe councils and local wildfire programmes. Together, these sources shape enforceable standards to make future communities more resilient.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4 - LA Fires</span>"
    ]
  },
  {
    "objectID": "Week 4 - LA Fires.html#application",
    "href": "Week 4 - LA Fires.html#application",
    "title": "4  Week 4 - LA Fires",
    "section": "4.2 Application:",
    "text": "4.2 Application:\nFRAP already utilises remotely sensed data to monitor fires, designate hazard risk areas and evaluate the impacts of fires. In 2025 they developed the California Vegetation Burn Severity dataset (their latest dataset) to meet the requirements of the Senate Bill 1101. The dataset estimates fire severity in California between 2015 and 2023 using remotely sensed data. Fire perimeters were taken from FRAP’s Historic Wildland Fire Perimeters database and uploaded to Google Earth Engine (GEE). Within each perimeter, pre- and post-fire Landsat imagery (June–September period) were processed. These generated the Relative differenced Normalised Burn Ratio (RdNBR), which measures vegetation greenness change, and a bias-corrected Composite Burn Index (CBI), predicted from RdNBR using regression equations based on field plots.The resulting continuous rasters were clipped to fire boundaries (and to forest types for CBI), reclassified into severity classes, and converted to vector layers.\nAlthough the U.S. has extensive remote sensing systems for wildfire monitoring and response, their application in predicting, preventing wildfires, and supporting evacuations remains limited.\n\n4.2.0.1 I propose a methodology to support wildfire prediction and mitigation efforts using remotely sensed data:\nThis method uses Sentinel-2 vegetation indices to map areas with dry vegetation that could fuel wildfires, producing near real-time fuel stress maps and drought time series. VIIRS and Sentinel-1 track wildfire spread and direction, while building footprints, traffic, and population data identify high-risk housing areas and potential evacuation bottlenecks. By combining remote sensing with GIS datasets, hazard risks can be visualised to support mitigation (e.g., controlled burning) and adaptation (e.g., efficient evacuation planning)\n\n\n\n\n\nThis method builds on Senate Bill 1101 by making wildfire risk mapping more dynamic and responsive. Instead of relying on static hazard zones, it uses regularly updated satellite data to show changing conditions. This means communities can plan mitigation and evacuation using information that reflects current risk.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4 - LA Fires</span>"
    ]
  },
  {
    "objectID": "Week 4 - LA Fires.html#reflection",
    "href": "Week 4 - LA Fires.html#reflection",
    "title": "4  Week 4 - LA Fires",
    "section": "4.3 Reflection:",
    "text": "4.3 Reflection:\nLA is a highly developed city and one of the United States’ largest economic engines. There are many good things you can say about LA despite the heat and traffic congestion. Therefore, they must have already developed a system that can predict wildfires, right? However, although extensive academic research has examined wildfire prediction through remotely sensed data (Sayad et al., 2019; Ntinopoulos et al., 2023), the city has yet to implement such methods, including the approach I proposed. While California employs remotely sensed data to evaluate wildfire impact and map hazard zones, the resulting product remains primarily static.\nUpon reflection, one potential improvement to my proposed approach would be the integration of machine learning and classification techniques to enhance wildfire prediction, as referenced in the methods above. Nonetheless, I believe this represents a strong first step toward producing a tangible outcome within a relatively short time frame, as it avoids the need for intensive analysis. Moreover, since FRAP has already utilized GEE for various visualisations, adopting this approach should be relatively feasible.\nFinally, while Senate Bill 1101 represents a step forward by requiring CAL FIRE to develop products, the bill leaves a major gap in how these mandates will be carried out. It sets ambitious deadlines but does not guarantee the funding, staffing, or technical resources necessary to implement them effectively. This risks creating a cycle where outputs such as maps and reports are produced to satisfy compliance, but without the capacity to translate them into tangible wildfire resilience. In this sense, the policy is strong on vision but not on execution, limiting its potential impact.\nWhat resonates with me from going through the policy, as an apprentice data scientist, is how often requests from senior leaders arrive with only broad direction. For instance, I might be asked to explore council priorities or measure returns on investment, but without clear guidance on what specifically should be examined or how results should be presented. This can leave analysts spending considerable time trying to interpret intentions rather than focusing on the analysis itself. Although those who do not work with remote sensing data may not grasp the technical details, more communication between decision makers and researchers can help bridge that gap. Leaders do not need to understand every step, but clearer goals and expectations make the analysis more relevant and useful.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4 - LA Fires</span>"
    ]
  },
  {
    "objectID": "Week 5 - GEE.html",
    "href": "Week 5 - GEE.html",
    "title": "5  Week 5 - GEE",
    "section": "",
    "text": "5.1 Reflection:\nThis week has been very interesting since I have learned how to use a new software (GEE)and coding language (Javascript) which isn’t as difficult as I thought it would be.\nThe key value for me is not just speed, but the way GEE removes technical barriers: I can interrogate decades of Landsat and Sentinel imagery without relying on expensive hardware. This is directly relevant to my work for the council where fast and reliable analysis is needed for planning and environmental decisions but we don’t have the computing power to do this effectively. Likewise, as a student I can access GEE for free, but in a commercial setting the council would need to pay, which could be a barrier to using it.\nAlthough, I am thinking about how I could use it for my dissertation. Tower Hamlets is densely built and has limited green space, so quantifying the cooling effect of vegetation is critical. Using indices such as NDVI or land surface temperature products, I could map where green cover has declined and overlay this with demographic data to identify communities most exposed to urban heat. Similarly, GEE could support routine monitoring of air quality proxies, flood-risk surfaces, or changes in permeable land cover linked to new developments. These are tasks that would be slow and resource-intensive using conventional GIS, but scalable in GEE.\nThat said, the platform does not eliminate uncertainty. Broad land-cover categories may mask the differences between residential, commercial, and industrial zones within Tower Hamlets, leading to oversimplified conclusions. Likewise, the coding error highlighted in the fire severity study demonstrates that small mistakes can have large consequences. For local governments, this means any GEE analysis must be accompanied by validation against ground truth or higher-resolution datasets.\nIn the future, I see potential in combining GEE with machine-learning methods for predictive modelling, such as forecasting where heat risk will intensify under projected land-use changes. This makes the tool not just a way of analysing historical data but a means of shaping policy in Tower Hamlets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5 - GEE</span>"
    ]
  },
  {
    "objectID": "Week 5 - GEE.html#summary",
    "href": "Week 5 - GEE.html#summary",
    "title": "5  Week 5 - GEE",
    "section": "",
    "text": "5.1.1 Machine learning:\nHumans by nature use inductive learning: our brain provides us with information from experiences which we use to derive patterns. The more past experiences we have the better prediction we make.\nMachine learning is the science of a computer modelling the human learning process.\nThere are various types of machine learning:\nSupervised learning: Learning the function that relates an input and output using a training example given as input and output pairs.\nUnsupervised learning: Learns patterns in a data set without reference to training data\nBelow I break down some the methods we have discussed in the lecture in the best way I can.. Through a table (not all otherwise we’d be here forever): \n\n\n\n\n\n\n\n\n\n\nName\nType\nMethod\nStrengths\nLimitations\n\n\n\n\nClassification and Regression Trees (CART)\nSupervised\nBuilds a decision tree by splitting data into groups based on key features; final leaves give predicted class (categorical) or value (continuous).\nEasy to understand & interpret; handles both categorical & continuous data.\nProne to overfitting; unstable to small changes; lower accuracy than ensembles (e.g., Random Forests).\n\n\nRandom Forests\nSupervised\nEnsemble of CART trees built from bootstrap samples and random feature subsets; predictions combined by majority vote (classification) or averaging (regression).\nHigh accuracy & robustness; handles large datasets; reduces overfitting.\nLess interpretable (“black box”); computationally heavier; requires parameter tuning.\n\n\nk-means\nUnsupervised\nGroups pixels into k clusters by assigning them to the nearest cluster center and adjusting centers until stable.\nSimple & fast; works well with compact, well-separated clusters; no training data needed.\nMust predefine k; sensitive to initial placement; struggles with irregular or noisy clusters.\n\n\n\n\nSome key points I took from this week’s lecture:\nWith higher accuracy we often lose interpretability of our models.\n\n\n\n\n\nSource: SHEYKHMOUSA et al. 2020 Support Vector Machine Versus Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review\nWhen running decision trees - we can see every decision that it makes.\nWhereas with Random Forests - we don’t know predictions are made because while they are often accurate these models are also very complex.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5 - GEE</span>"
    ]
  },
  {
    "objectID": "Week 5 - GEE.html#applications",
    "href": "Week 5 - GEE.html#applications",
    "title": "5  Week 5 - GEE",
    "section": "5.2 Applications:",
    "text": "5.2 Applications:\n\n5.2.1 Supervised method:\n\n\n\nFramework of the proposed method for mangrove ecosystem mapping using the random forest (RF)\n\n\nGhorbanian et al. (2021), mapped the Hara mangrove ecosystem using Sentinel-1 synthetic aperture radar data and Sentinel-2 optical data within Google Earth Engine. Nine detailed mangrove classes were identified using precise reference samples from high-resolution imagery,divided into training and testing sets with cross-validation to ensure robustness. Seasonal features reflecting tidal variations were used with a Random Forest classifier, improving classification accuracy. The approach achieved high accuracy (93.23% overall accuracy, 0.92 Kappa coefficient) and consistency, demonstrating the effectiveness of combining multi-source data and seasonal downscaling for detailed, automated, and repeatable mangrove ecosystem mapping worldwide. Supervised classification was ideal due to difficult field access, necessitating reliable reference data from imagery, and the need to differentiate detailed mangrove classes critical for ecosystem monitoring.\n\n\n5.2.2 Unsupervised:\nOn the other hand Rapinela et al. (2019) used an unsupervised hierarchical TWINSPAN classifier to group vegetation relevé data into plant community types. TWINSPAN worked by repeatedly splitting the data into clusters based on species composition and identifying indicator species that defined each division. This process produced a hierarchical classification tree of vegetation units, with each relevé assigned to a cluster of similar species assemblages. The quality of these clusters was then assessed using silhouette widths, where values close to 1 indicated strong cohesion within clusters and clear separation from others. This method effectively identified consistent, locally relevant plant communities in wet grasslands, avoiding the limitations of broad, continental-scale habitat maps that often overlook fine-scale ecological patterns.\n\n\n\nMap of plant communities derived from the classification of Sentinel-2 time-series using a linear kernel support vector machine classifier.\n\n\nUltimately, Ghorbanian et al. (2021) showed supervised methods achieve high accuracy but rely on reference data and heavy computation. Rapinela et al. (2019) highlighted that unsupervised approaches avoid this but are less transferable. Future work could explore hybrid or semi-supervised methods (Which I’m sure we will cover next week), combining the precision of supervised models with the adaptability of unsupervised ones, supported by advances in transfer learning and cloud-based platforms.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5 - GEE</span>"
    ]
  },
  {
    "objectID": "Week 5 - GEE.html#reflection",
    "href": "Week 5 - GEE.html#reflection",
    "title": "5  Week 5 - GEE",
    "section": "5.3 Reflection:",
    "text": "5.3 Reflection:\nThis week has been very interesting since I have learned how to use a new software (GEE)and coding language (Javascript) which isn’t as difficult as I thought it would be. \nThe key value for me is not just speed, but the way GEE removes technical barriers: I can interrogate decades of Landsat and Sentinel imagery without relying on expensive hardware. This is directly relevant to my work for the council where fast and reliable analysis is needed for planning and environmental decisions but we don’t have the computing power to do this effectively. Likewise, as a student I can access GEE for free, but in a commercial setting the council would need to pay, which could be a barrier to using it.\nAlthough, I am thinking about how I could use it for my dissertation. Tower Hamlets is densely built and has limited green space, so quantifying the cooling effect of vegetation is critical. Using indices such as NDVI or land surface temperature products, I could map where green cover has declined and overlay this with demographic data to identify communities most exposed to urban heat. Similarly, GEE could support routine monitoring of air quality proxies, flood-risk surfaces, or changes in permeable land cover linked to new developments. These are tasks that would be slow and resource-intensive using conventional GIS, but scalable in GEE.\nThat said, the platform does not eliminate uncertainty. Broad land-cover categories may mask the differences between residential, commercial, and industrial zones within Tower Hamlets, leading to oversimplified conclusions. Likewise, the coding error highlighted in the fire severity study demonstrates that small mistakes can have large consequences. For local governments, this means any GEE analysis must be accompanied by validation against ground truth or higher-resolution datasets.\nIn the future, I see potential in combining GEE with machine-learning methods for predictive modelling, such as forecasting where heat risk will intensify under projected land-use changes. This makes the tool not just a way of analysing historical data but a means of shaping policy in Tower Hamlets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5 - GEE</span>"
    ]
  },
  {
    "objectID": "Week 6 - Classification 1.html",
    "href": "Week 6 - Classification 1.html",
    "title": "6  Week 6 - Classification 1",
    "section": "",
    "text": "6.1 Summary\nBefore the lecture, I felt nervous as machine learning and classification seemed challenging. However, unlike GEE, these techniques have long been used in remote sensing (Lary, 2015) to categorise pixels, objects, or patterns into thematic classes. Here is a very interesting example where a nonprofit organisation used satellite technology and machine learning to combat illegal fishing:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6 - Classification 1</span>"
    ]
  },
  {
    "objectID": "Week 6 - Classification 1.html#summary",
    "href": "Week 6 - Classification 1.html#summary",
    "title": "6  Week 6 - Classification 1",
    "section": "",
    "text": "6.1.1 Machine learning:\nHumans by nature use inductive learning: our brain provides us with information from experiences which we use to derive patterns. The more past experiences we have the better prediction we make.\nMachine learning is the science of a computer modelling the human learning process.\nThere are various types of machine learning:\nSupervised learning: Learning the function that relates an input and output using a training example given as input and output pairs.\nUnsupervised learning: Learns patterns in a data set without reference to training data\nBelow I break down some the methods we have discussed in the lecture in the best way I can.. Through a table (not all otherwise we’d be here forever): \n\n\n\n\n\n\n\n\n\n\nName\nType\nMethod\nStrengths\nLimitations\n\n\n\n\nClassification and Regression Trees (CART)\nSupervised\nBuilds a decision tree by splitting data into groups based on key features; final leaves give predicted class (categorical) or value (continuous).\nEasy to understand & interpret; handles both categorical & continuous data.\nProne to overfitting; unstable to small changes; lower accuracy than ensembles (e.g., Random Forests).\n\n\nRandom Forests\nSupervised\nEnsemble of CART trees built from bootstrap samples and random feature subsets; predictions combined by majority vote (classification) or averaging (regression).\nHigh accuracy & robustness; handles large datasets; reduces overfitting.\nLess interpretable (“black box”); computationally heavier; requires parameter tuning.\n\n\nk-means\nUnsupervised\nGroups pixels into k clusters by assigning them to the nearest cluster center and adjusting centers until stable.\nSimple & fast; works well with compact, well-separated clusters; no training data needed.\nMust predefine k; sensitive to initial placement; struggles with irregular or noisy clusters.\n\n\n\n\nSome key points I took from this week’s lecture:\nWith higher accuracy we often lose interpretability of our models.\n\n\n\n\n\nSource: SHEYKHMOUSA et al. 2020 Support Vector Machine Versus Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review\nWhen running decision trees - we can see every decision that it makes.\nWhereas with Random Forests - we don’t know predictions are made because while they are often accurate these models are also very complex.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6 - Classification 1</span>"
    ]
  },
  {
    "objectID": "Week 6 - Classification 1.html#applications",
    "href": "Week 6 - Classification 1.html#applications",
    "title": "6  Week 6 - Classification 1",
    "section": "6.2 Applications:",
    "text": "6.2 Applications:\n\n6.2.1 Supervised method:\n\n\n\nFramework of the proposed method for mangrove ecosystem mapping using the random forest (RF)\n\n\nGhorbanian et al. (2021), mapped the Hara mangrove ecosystem using Sentinel-1 synthetic aperture radar data and Sentinel-2 optical data within Google Earth Engine. Nine detailed mangrove classes were identified using precise reference samples from high-resolution imagery,divided into training and testing sets with cross-validation to ensure robustness. Seasonal features reflecting tidal variations were used with a Random Forest classifier, improving classification accuracy. The approach achieved high accuracy (93.23% overall accuracy, 0.92 Kappa coefficient) and consistency, demonstrating the effectiveness of combining multi-source data and seasonal downscaling for detailed, automated, and repeatable mangrove ecosystem mapping worldwide. Supervised classification was ideal due to difficult field access, necessitating reliable reference data from imagery, and the need to differentiate detailed mangrove classes critical for ecosystem monitoring.\n\n\n6.2.2 Unsupervised:\nOn the other hand Rapinela et al. (2019) used an unsupervised hierarchical TWINSPAN classifier to group vegetation relevé data into plant community types. TWINSPAN worked by repeatedly splitting the data into clusters based on species composition and identifying indicator species that defined each division. This process produced a hierarchical classification tree of vegetation units, with each relevé assigned to a cluster of similar species assemblages. The quality of these clusters was then assessed using silhouette widths, where values close to 1 indicated strong cohesion within clusters and clear separation from others. This method effectively identified consistent, locally relevant plant communities in wet grasslands, avoiding the limitations of broad, continental-scale habitat maps that often overlook fine-scale ecological patterns.\n\n\n\nMap of plant communities derived from the classification of Sentinel-2 time-series using a linear kernel support vector machine classifier.\n\n\nUltimately, Ghorbanian et al. (2021) showed supervised methods achieve high accuracy but rely on reference data and heavy computation. Rapinela et al. (2019) highlighted that unsupervised approaches avoid this but are less transferable. Future work could explore hybrid or semi-supervised methods (Which I’m sure we will cover next week), combining the precision of supervised models with the adaptability of unsupervised ones, supported by advances in transfer learning and cloud-based platforms.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6 - Classification 1</span>"
    ]
  },
  {
    "objectID": "Week 6 - Classification 1.html#reflection",
    "href": "Week 6 - Classification 1.html#reflection",
    "title": "6  Week 6 - Classification 1",
    "section": "6.3 Reflection:",
    "text": "6.3 Reflection:\nMachine learning has always felt inaccessible, overly technical, and far removed from practical application. However, this week changed my perspective. Rather than seeing algorithms as abstract, I began to recognise them as structured ways of organising uncertainty, enabling us to generate knowledge from complex datasets. This allowed me to move from nervousness toward curiosity.\nWhat I found most engaging was the relevance of these methods across very different contexts. A week before the lecture, I had already tried applying K-means clustering on census data to group LSOAs in Tower Hamlets that share characteristics of poor recycling according toReLondon (The Mayor of London’s Recycling Team). I admit that I wasn’t really 100% sure what I was doing so I relied heavily on google. This was the output:\n\n\n\n\n\nbut on reflection I now recognise that K-means may not be the most appropriate method for polygon data, as it is better suited to continuous feature space rather than spatial units with irregular boundaries.\nI also started to think about other ways I could apply classification and remote sensing to work. For instance, supervised learning might help map green space distribution or track patterns of development, while unsupervised approaches could identify emerging trends without the burden of extensive training data. Such possibilities suggest real scope for evidence based decision making at a local level.\nThe lecture also raised a critical methodological point. If training and validation data are located too closely together. This spatial dependency can lead to inflated accuracy, effectively giving the model a “sneak peek” at validation data. Unless this is accounted for through techniques such as spatial cross validation or block sampling, results may appear stronger than they truly are. The risk is particularly important in policy contexts, where misleading accuracy could undermine trust or lead to poor decisions.\nOverall, I think that this week was less about memorising algorithms and more about understanding their implications. Machine learning is not simply a technical skillset but a bridge between disciplines. It holds potential for environmental monitoring, urban planning, and social policy, provided it is applied with both rigour and transparency.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6 - Classification 1</span>"
    ]
  },
  {
    "objectID": "Week 7 - Classification 2.html",
    "href": "Week 7 - Classification 2.html",
    "title": "7  Week 7 - Classification 2",
    "section": "",
    "text": "7.1 Summary:\nFor local analysis nothing really beats doing your own classification for that particular time point. A statement from this week’s lecture that really stuck with me. This was said when we explored the dynamic world model, which uses a semi-supervised approach to produce a global 10m resolution near real time land cover dataset. It uses surface reflectance data (which is better quality) for labelling but Top of Atmosphere for the model as it includes data from before 2017, which surface reflectance does not. The model also uses a convolutional neural network which uses an algorithm to recognise patterns in data. However, the convolutional model also does not give information about which training data is most useful. While it is useful for broader insights, if working in a local study area it is better to use your own classification or a national product.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7 - Classification 2</span>"
    ]
  },
  {
    "objectID": "Week 7 - Classification 2.html#summary",
    "href": "Week 7 - Classification 2.html#summary",
    "title": "7  Week 7 - Classification 2",
    "section": "",
    "text": "7.1.0.1 This week we covered: \n\nObject-based analysis which is based on the idea that pixels are not representative of real life. Therefore groups similar pixels into meaningful vector objects which are then used to classify land cover types. However, OBIA relies on segmentation so small changes in parameters can highly impact the results. \nWhereas, sub-pixel analysis does not consider a pixel to be homogeneous and considers a pixel to be made up of many components so attempts to calculate the proportion of land cover in a pixel itself. To do this we select spectrally pure end members for each land cover type and calculate fractions of each land cover inside the pixel. Which in my opinion is a better method because even 10m pixels are still likely to be heterogeneous. \n\nWe also covered accuracy because after producing an output we need to assign an accuracy value to it. \n\n\n7.1.0.2 In remote sensing we focus on: \nProducer accuracy - where the classification results meet the expectation of the creator.\nUser accuracy - how representative the classification is of reality. \nOverall accuracy \nIn today’s lecture we covered various accuracy assessments including Kappa, F1 and the receiver operating characteristic curve. One thing that they all had in common was:\nTHEY DO NOT ACCOUNT FOR SPATIAL DEPENDENCY! \n‘Training’ observations near the ‘test’ observations can provide a kind of ‘sneak preview’: information that should be unavailable to the training dataset (Lovelace et al, 2022). So one thing I do know going forward is to always conduct spatial cross validation on remote sensing (as they are a spatial dataset).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7 - Classification 2</span>"
    ]
  },
  {
    "objectID": "Week 8 - Temperature.html",
    "href": "Week 8 - Temperature.html",
    "title": "8  Week 8 - Temperature",
    "section": "",
    "text": "8.1 Summary:\nIn week 5 we covered temperature and policy as my group in CASA0025 are interested in producing a heat based application, I was particularly interested in the temperature aspect.\nOne thing that I have learned from working in local government is that analysis needs to have a purpose. I can’t be researching something just because it is interesting as that is not a good use of time or the council’s money.\nThe problem in this case is the Urban heat Island (UHI) Effect - this is where urban areas obtain comparatively higher atmospheric and surface temperatures than surrounding rural areas.\nThere are two main reasons for UHI’s:\nHowever many serious impacts on people and the environment:\nThe body’s inability to regulate internal temperature and eliminate heat gain increases the risk of heatstroke.\nThe strain put on the body as it tries to cool itself also stresses the heart and kidneys, potentially worsening health risk from chronic conditions (cardiovascular, mental, respiratory and diabetes related conditions.\nHeat can also disrupt and compromise essential health services, such as the loss of power supply and transport.\nThere is more information available on the world health organisations website.\nFor our final learning diary entry we had the choice to choose between temperature and SAR. The reason I chose temperature was because it hits close to home. London is becoming increasingly hotter over the years and not only will it negatively impact vulnerable members of the community (my grandparents) but it will also become more extreme and threaten future generations.\nIn this weeks practical we explored the main sources of remotely sensed temperature data:\nModerate Resolution Imaging Spectroradiometer (MODIS):\nMODIS is an instrument aboard both the Terra and Aqua satellites. Terra crosses the equator from N to S in the morning and Aqua S to N in the afternoon meaning the entire earth is sampled every 1-2 days, with most places getting 2 images a day. It has a resolution of 1km.\nLandsat:\nThe temperature band in Landsat is B10 and as we already know it has a resolution of 30m and has a 16 day revisit cycle.\nAlthough MODIS has a higher collection of images due to its resolution I am more likely to use Landsat as my analysis is mostly local (either the area of Tower Hamlets or London) and MODIS would not capture the spatial heterogeneity of these areas. For our casa0025 assessment we chose MODIS for this reason.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8 - Temperature</span>"
    ]
  },
  {
    "objectID": "Week 7 - Classification 2.html#application",
    "href": "Week 7 - Classification 2.html#application",
    "title": "7  Week 7 - Classification 2",
    "section": "7.2 Application:",
    "text": "7.2 Application:\nIn 2011 Moskal et al., published a study which used OBIA to classify land use and land cover, particularly focusing on urban tree cover in Seattle. The study utilised publicly available hyperspectral imagery from the National Agricultural Imagery Program and QuickBird (used for case study 3). It also used ancillary GIS datasets including parcel shapefiles, roads and buildings footprints. They applied a hierarchical approach using primary segmentation to separate major land cover classes, such as vegetation and impervious surfaces, using spectral, spatial, and contextual information. Within each primary class, they further subdivided the objects using different parameters (e.g., texture, shape, size) to better delineate specific features, like individual tree crowns or smaller patches of vegetation. Rule based classification was then applied to assign land cover class to extracted features. \nThe image below displays (a) 2002 and (b) 2009 OBIA drive classifications. Where: dark green represents tree cover, light green represents grass and fields and all other colours are impervious surfaces (grey for roads and paved areas, light brown for ground and brown for buildings.\n\n\n\n\n\nTo assess the accuracy of each case study, a visual assessment using 2.54 cm resolution, 2002 aerial imagery combined with the 2009 co-registered oblique photography was conducted by an expert image analyst who was not involved in the classification process. The image below represents a confusion matrix similar to the ones we discussed in the lecture to display the accuracy assessment for case study 1.\n\n\n\nThe overall accuracy was 71.3% which is reasonably good for broad analysis, however may not be useful for fine scale analysis. Segmentation often over- or under-represented features, while spectral confusion between trees, grass, and shadows reduced accuracy. The 71.3% overall accuracy also meant nearly one in three features were misclassified. Likewise, the assessment was undertaken by a single analyst. \nGhorbanzadeh et al., however aimed to improve the detection of landslides using a combination of deep learning and OBIA. They first generated landslide probability maps with ResU-Net, which identified potential landslide areas based on pixel-level classification. then applied rule-based OBIA using geometrical features including the length-to-width ratio to distinguish between landslides and other similar looking features such as riverbeds. By integrating both approaches, they produced more accurate landslide maps, significantly improving detection performance compared to using each method alone.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7 - Classification 2</span>"
    ]
  },
  {
    "objectID": "Week 7 - Classification 2.html#reflection",
    "href": "Week 7 - Classification 2.html#reflection",
    "title": "7  Week 7 - Classification 2",
    "section": "7.3 Reflection:",
    "text": "7.3 Reflection:\nAs much as it has taken a while to wrap my head around classifications, I feel like I am finally starting to get the hang of it. I understand the theory behind OBIA and sub pixel analysis however when putting it into practice during the practical, I felt like my brain was about to explode (It was a lot to take in at once!)\nAlthough I did finally make it to the end of the practical and produced a CART classified map layer. I still would need a lot more time to make sense of the code and more practice in google earth engine. \n\n\n\nFor now, based on what I understand from the material, if I were to choose a classification method I would opt for sub pixel analysis because with 30m landsat data like the one used for this week’s practical many pixels would include mixtures of land cover types. OBIA push each pixel into one class which would not work for heterogeneous blocks.\nAt work, these methods could be applied to monitoring how green space within urban areas changes over time. The council are often researching leisure and physical activity of residents in the borough and the topic of proximity to greenspace has come up. Likewise, Tower Hamlets is also rapidly growing in population and new developments are often being built to adapt to this. Being able to distinguish between mixed land cover types would make it easier to track whether areas that appear as open space are gradually being lost to development or whether vegetation quality within parks is declining.\nLinks:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7 - Classification 2</span>"
    ]
  },
  {
    "objectID": "Week 8 - Temperature.html#summary",
    "href": "Week 8 - Temperature.html#summary",
    "title": "8  Week 8 - Temperature",
    "section": "",
    "text": "More dark surfaces that retain heat\nLess vegetation that cools the environment (evapotranspiration and solar blocking).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8 - Temperature</span>"
    ]
  },
  {
    "objectID": "Week 8 - Temperature.html#application",
    "href": "Week 8 - Temperature.html#application",
    "title": "8  Week 8 - Temperature",
    "section": "8.2 Application:",
    "text": "8.2 Application:\nAfter this practical I remembered the list of awesome GEE apps we were given in week 5 List of awesome GEE apps. \nSo naturally I went searching for an LST application and found one.. \nThis application by vaishnaviadhav31 displays LST during the day in Pune, India accompanied with a timeseries. Presumably, they used MODIS as the resolution is 1km but it seems to have somewhat worked here as some differences in temperature across the city are evident. This application and others gave me a good idea of how web apps can be presented and gave me a bit more inspiration for the casa0025 module so I shared it with my group during a meeting we had to discuss the mockup of our heat vulnerability index application. Although we still have not decided whether we are using GEE  to build our app or a python library called streamlit… \n\n\n\n\n\nConversely, In 2014 Weng et al., introduced SADFAT (Spatio-temporal Adaptive Data Fusion Algorithm for Temperature mapping), an innovative approach designed to produce high-resolution, daily Land Surface Temperature (LST) maps by integrating MODIS data with Landsat imagery. Their goal was to overcome the temporal–spatial trade-off inherent in satellite observations, where MODIS provides high temporal but coarse spatial resolution, and Landsat provides fine spatial but infrequent temporal coverage.\nLandsat Thematic Map images were scaled, geometrically aligned, and resampled to match the MODIS spatial resolution. MODIS daily LST and reflectance data were then reprojected to the Landsat coordinate system using the MODIS Reprojection Tools. The SADFAT method involved first identifying spectrally similar pixels within a local window, using spectral thresholds to ensure that selected pixels shared comparable land cover characteristics. Weights were then assigned to these pixels based on both spectral similarity and temporal proximity, enabling the integration of multi-date information. Next, regression analysis was used to estimate conversion coefficients that linked radiance changes between MODIS and Landsat observations over time. These coefficients were then applied to MODIS radiance data at the prediction date, and the derived radiance values were converted into temperature estimates using Planck’s law. Finally, to enhance reliability, pixels with low confidence were masked out.\nThis fusion approach was tested in Los Angeles County, California, with results showing that predicted LST errors ranged from about 1.3 K to 2 K across five dates in 2005. Such accuracy demonstrates the effectiveness of SADFAT in reconciling the trade-off between spatial and temporal resolution. The method clearly enhances the spatial detail of thermal data while retaining the high temporal frequency of MODIS, making it especially valuable for applications such as urban heat island analysis, environmental monitoring, and climate impact assessments.\n\n\n\n\n\nNevertheless, the study also revealed some limitations. While SADFAT performed well in general, it struggled to fully capture rapid or unrecorded temperature fluctuations, which may occur due to transient weather conditions, localized anthropogenic activities, or sensor acquisition gaps. Despite these constraints, the approach marked a significant advancement in thermal remote sensing fusion, enabling researchers to investigate temperature dynamics at scales directly relevant to ecological processes, urban planning strategies, and public health concerns.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8 - Temperature</span>"
    ]
  },
  {
    "objectID": "Week 8 - Temperature.html#reflection",
    "href": "Week 8 - Temperature.html#reflection",
    "title": "8  Week 8 - Temperature",
    "section": "8.3 Reflection:",
    "text": "8.3 Reflection:\nWhat a great way to end the module! This week has been the most interesting and useful for me because not only am I using temperature data for casa0025 but I am also particularly interested in exploring heat related health risks in Tower Hamlets for my dissertation. I am not 100% sure what I’ll be doing yet as I am on a part time course, however I hope to utalise sensor data from the council and landsat LST to predict the risk of overheating in flats, comparing permitted development flats with ones that have undergone regular planning applications. Permitted development flats are known to be death traps among the council and there has been extensive research on the negative impacts on these builds but limited on their relationship with health and heat. I really want to make an impact with my dissertation and being able to identify heat risk in a borough with some of the most vulnerable populations will in no doubt do so. Again the reason why I am most likely to use Landsat is because of its lower resolution compared to MODIS. Once I have done more research I may change my method for instance integrating MODIS with Landsat or using data that i am not currently aware of. \nMoreover, although I am not a pro at using GEE yet I had some fun with replicating the BBC heat Index on census tracts in Los Angeles. \nhttps://www.who.int/news-room/fact-sheets/detail/climate-change-heat-and-health\n\n\n\n\n\nI have been playing around with it every now and again so I don’t forget what I have learned and it’s been giving me a lot more confidence with using the software.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8 - Temperature</span>"
    ]
  }
]